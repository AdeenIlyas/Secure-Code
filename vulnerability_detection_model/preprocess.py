import pandas as pd  # Data manipulation
from sklearn.model_selection import train_test_split  # Data splitting
from sklearn.utils import shuffle  # Data shuffling
from tensorflow.keras.preprocessing.text import Tokenizer  # Text tokenization
from tensorflow.keras.preprocessing.sequence import pad_sequences  # Sequence padding
import pickle  # Object serialization

# Load dataset
df = pd.read_csv('vulnerability_fix_dataset.csv')  # Load vulnerability dataset

# Create labeled data
vulnerable_df = df[['vulnerable_code']].copy()  # Extract vulnerable code
vulnerable_df['label'] = 1  # Label as vulnerable (1)
vulnerable_df.rename(
    columns={'vulnerable_code': 'code'}, inplace=True)  # Rename column

non_vulnerable_df = df[['fixed_code']].copy()  # Extract fixed code
non_vulnerable_df['label'] = 0  # Label as non-vulnerable (0)
non_vulnerable_df.rename(
    columns={'fixed_code': 'code'}, inplace=True)  # Rename column

# Concatenate and shuffle dataset
labeled_df = pd.concat([vulnerable_df, non_vulnerable_df],
                       ignore_index=True)  # Combine datasets
labeled_df.dropna(inplace=True)  # Remove empty entries
labeled_df = shuffle(labeled_df)  # Shuffle data

# Tokenization
tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')  # Create tokenizer
tokenizer.fit_on_texts(labeled_df['code'])  # Fit on code text

sequences = tokenizer.texts_to_sequences(
    labeled_df['code'])  # Convert to sequences
max_length = 500  # Maximum sequence length
padded_sequences = pad_sequences(
    sequences, maxlen=max_length, padding='post', truncating='post')  # Pad sequences

# Save tokenizer for later use
with open('tokenizer.pkl', 'wb') as f:  # Save tokenizer
    pickle.dump(tokenizer, f)

# Split dataset: 60% train, 20% validation, 20% test
X_train, X_temp, y_train, y_temp = train_test_split(  # First split: train vs temp
    padded_sequences, labeled_df['label'], test_size=0.4, random_state=42)

X_val, X_test, y_val, y_test = train_test_split(  # Second split: validation vs test
    X_temp, y_temp, test_size=0.5, random_state=42)

# Save preprocessed data
pickle.dump((X_train, y_train), open(
    'train_data.pkl', 'wb'))  # Save training data
pickle.dump((X_val, y_val), open('val_data.pkl', 'wb'))  # Save validation data
pickle.dump((X_test, y_test), open('test_data.pkl', 'wb'))  # Save test data

print("Preprocessing complete and data saved successfully.")  # Success message
